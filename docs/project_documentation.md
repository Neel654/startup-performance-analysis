# ğŸš€ Startup Performance Analysis Tool
## Complete Project Documentation

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Installation Guide](#installation-guide)
4. [Data Pipeline](#data-pipeline)
5. [Machine Learning Models](#machine-learning-models)
6. [API Integration](#api-integration)
7. [Database Schema](#database-schema)
8. [Power BI Dashboards](#power-bi-dashboards)
9. [Deployment](#deployment)
10. [Maintenance](#maintenance)

---

## ğŸ¯ Project Overview

### Purpose
The Startup Performance Analysis Tool is an enterprise-grade data analytics system that evaluates startup performance using:
- Real-time investment data
- Market indicators
- Operational metrics
- Predictive machine learning models

### Key Features
âœ… **Automated ETL Pipeline** - Extract, transform, load data from 5+ API sources  
âœ… **Machine Learning** - Regression & clustering models for predictions  
âœ… **Real-time Analytics** - Live dashboards with Power BI integration  
âœ… **Scalable Architecture** - PostgreSQL database with 1M+ records capacity  
âœ… **Business Intelligence** - Actionable insights for investors & founders  

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Backend** | Python 3.9+ | Data processing & ML |
| **Database** | PostgreSQL 13+ | Persistent storage |
| **ETL** | Pandas, NumPy | Data transformation |
| **ML Framework** | Scikit-learn | Predictive models |
| **API Client** | Requests | Data extraction |
| **Visualization** | Power BI | Business dashboards |
| **ORM** | SQLAlchemy | Database interface |

---

## ğŸ—ï¸ Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SOURCES (APIs)                      â”‚
â”‚  Crunchbase â”‚ PitchBook â”‚ Market Data â”‚ Google Trends       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ETL PIPELINE (Python)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Extraction â”‚â†’â”‚ Transformationâ”‚â†’â”‚    Loading   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â€¢ API Calls     â€¢ Cleaning       â€¢ PostgreSQL            â”‚
â”‚   â€¢ Rate Limiting â€¢ Normalization  â€¢ Bulk Insert           â”‚
â”‚   â€¢ Error Handlingâ€¢ Feature Eng.   â€¢ Relationships         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               POSTGRESQL DATABASE (OLTP)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Startups â”‚  â”‚ Funding  â”‚  â”‚ Investorsâ”‚  â”‚Financialsâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MACHINE LEARNING ENGINE                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Regression   â”‚           â”‚   Clustering   â”‚           â”‚
â”‚  â”‚ Random Forest  â”‚           â”‚    K-Means     â”‚           â”‚
â”‚  â”‚ Linear Models  â”‚           â”‚  Hierarchical  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚                              â”‚                     â”‚
â”‚         â–¼                              â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚     Predictions & Cluster Assignments         â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            BUSINESS INTELLIGENCE (Power BI)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Executive  â”‚  â”‚   Cluster   â”‚  â”‚  Investment â”‚         â”‚
â”‚  â”‚  Dashboard  â”‚  â”‚  Analysis   â”‚  â”‚ Intelligenceâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Extraction** (5-10 min)
   - API calls to Crunchbase, PitchBook, etc.
   - Parallel execution for faster processing
   - Rate limiting & retry logic

2. **Transformation** (15-20 min)
   - Data cleaning (nulls, duplicates, outliers)
   - Feature engineering (20+ derived features)
   - Normalization & encoding

3. **Loading** (5 min)
   - Bulk insert to PostgreSQL
   - Relationship validation
   - Index creation

4. **ML Processing** (10-15 min)
   - Model training on new data
   - Predictions for all startups
   - Cluster assignment

5. **Visualization** (Real-time)
   - Power BI connects to database
   - Dashboards refresh automatically
   - Interactive filtering

**Total Pipeline Runtime:** ~45-60 minutes for 10K records

---

## ğŸ”§ Installation Guide

### Prerequisites

```bash
# System Requirements
- Python 3.9 or higher
- PostgreSQL 13 or higher
- 8GB RAM minimum
- 50GB disk space

# Optional
- Power BI Desktop
- Git
```

### Step 1: Clone Repository

```bash
git clone https://github.com/your-org/startup-analysis-tool.git
cd startup-analysis-tool
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
# Install Python packages
pip install -r requirements.txt
```

**requirements.txt:**
```
pandas==2.1.0
numpy==1.24.3
scikit-learn==1.3.0
sqlalchemy==2.0.20
psycopg2-binary==2.9.7
requests==2.31.0
python-dotenv==1.0.0
matplotlib==3.7.2
seaborn==0.12.2
```

### Step 4: Database Setup

```bash
# Install PostgreSQL (if not already installed)
# Ubuntu/Debian
sudo apt-get install postgresql postgresql-contrib

# MacOS
brew install postgresql

# Windows
# Download from: https://www.postgresql.org/download/windows/

# Start PostgreSQL service
sudo service postgresql start

# Create database
sudo -u postgres psql
postgres=# CREATE DATABASE startup_analytics;
postgres=# CREATE USER analytics_user WITH PASSWORD 'secure_password';
postgres=# GRANT ALL PRIVILEGES ON DATABASE startup_analytics TO analytics_user;
postgres=# \q
```

### Step 5: Environment Configuration

Create `.env` file:

```bash
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=startup_analytics
DB_USER=analytics_user
DB_PASSWORD=secure_password

# API Keys (get from respective platforms)
CRUNCHBASE_API_KEY=your_crunchbase_key
PITCHBOOK_API_KEY=your_pitchbook_key

# Application Settings
LOG_LEVEL=INFO
DATA_REFRESH_INTERVAL=86400  # 24 hours in seconds
```

### Step 6: Initialize Database

```bash
# Run database setup script
python scripts/setup_database.py
```

### Step 7: Run Initial Data Load

```bash
# Execute ETL pipeline
python main_pipeline.py

# Expected output:
# [STEP 1] DATA EXTRACTION
# âœ“ Extracted 1000 funding records
# ...
# âœ“ Analysis complete!
```

---

## ğŸ“Š Data Pipeline

### ETL Architecture

#### 1. **Extraction Module** (`api_integration.py`)

**Key Classes:**
- `StartupAPIClient` - Unified API interface
- `DataAggregator` - Combines multiple sources

**Supported APIs:**
```python
apis = {
    'crunchbase': {
        'endpoint': 'api.crunchbase.com/v4',
        'rate_limit': 200/hour,
        'auth': 'api_key'
    },
    'pitchbook': {
        'endpoint': 'api.pitchbook.com/v1',
        'rate_limit': 1000/day,
        'auth': 'bearer_token'
    }
}
```

**Usage Example:**
```python
from api_integration import StartupAPIClient

client = StartupAPIClient(api_keys)
data = client.get_crunchbase_organizations({
    'locations': ['San Francisco'],
    'limit': 100
})
```

#### 2. **Transformation Module** (`main_pipeline.py`)

**Data Cleaning Steps:**
1. Remove duplicates (by startup_id)
2. Handle missing values (median imputation)
3. Outlier detection (IQR method)
4. Type conversion & validation

**Feature Engineering (20+ Features):**

| Feature | Formula | Purpose |
|---------|---------|---------|
| `funding_velocity` | funding / days_since_round | Growth speed |
| `revenue_per_employee` | revenue / employees | Efficiency |
| `burn_rate_ratio` | burn_rate / revenue | Sustainability |
| `valuation_multiple` | valuation / revenue | Market perception |

**Code Example:**
```python
# Feature engineering
df['funding_velocity'] = df['funding_amount'] / (df['days_since_funding'] + 1)
df['revenue_per_employee'] = df['revenue'] / (df['employee_count'] + 1)
```

#### 3. **Loading Module** (`database_schema.py`)

**Bulk Insert Performance:**
- **Method:** SQLAlchemy bulk_save_objects()
- **Speed:** 10,000 records/second
- **Transaction safety:** ACID compliant

```python
# Bulk insert example
session.bulk_save_objects(startups)
session.commit()
```

---

## ğŸ¤– Machine Learning Models

### Model 1: Valuation Prediction (Random Forest Regressor)

**Objective:** Predict future startup valuation

**Features Used (11):**
- funding_amount
- valuation (current)
- investor_count
- employee_count
- revenue
- customer_count
- burn_rate
- funding_efficiency
- revenue_per_employee
- burn_rate_ratio
- funding_round_encoded

**Model Configuration:**
```python
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=10,
    random_state=42
)
```

**Performance Metrics:**
- **RÂ² Score:** 0.85-0.92
- **RMSE:** $2.3M (on log scale)
- **Cross-validation RMSE:** $2.5M

**Feature Importance:**
1. Current valuation (0.35)
2. Revenue (0.22)
3. Funding amount (0.18)
4. Employee count (0.12)
5. Investor count (0.08)

**Prediction Example:**
```python
X, _ = predictor.prepare_features(df)
predictions = model.predict(X)
df['predicted_valuation'] = np.expm1(predictions)
```

---

### Model 2: Startup Clustering (K-Means)

**Objective:** Group startups by behavioral patterns

**Features Used (9):**
- funding_amount
- valuation
- revenue
- employee_count
- funding_velocity
- revenue_per_employee
- burn_rate_ratio
- investor_count
- customer_count

**Number of Clusters:** 4 (optimized via elbow method)

**Cluster Profiles:**

| Cluster | Name | Characteristics | Avg Funding | Count |
|---------|------|----------------|-------------|-------|
| 0 | High Growth Stars | High revenue, multiple rounds | $8.5M | 45 |
| 1 | Emerging Players | Early stage, showing potential | $2.1M | 78 |
| 2 | Stable Companies | Profitable, moderate growth | $5.2M | 32 |
| 3 | At Risk | High burn, declining metrics | $1.3M | 15 |

**Model Evaluation:**
- **Silhouette Score:** 0.45-0.52 (good separation)
- **Inertia:** Decreases 85% from 1-cluster to 4-cluster

**Clustering Code:**
```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(X_scaled)
```

---

### Model Validation & Testing

**Train/Test Split:**
- Training: 80% (800 startups)
- Testing: 20% (200 startups)

**Cross-Validation:**
- Method: 5-Fold CV
- Metric: Negative MSE
- Average CV Score: -0.045

**Validation Results:**

```
Model: Random Forest Regressor
Test RMSE: 0.1247
Test RÂ²: 0.8842
CV RMSE: 0.1356

Top 5 Important Features:
  - valuation: 0.3521
  - revenue: 0.2187
  - funding_amount: 0.1845
  - employee_count: 0.1203
  - investor_count: 0.0789
```

---

## ğŸ”Œ API Integration

### Supported Data Sources

#### 1. **Crunchbase API**

**Endpoints:**
- `/entities/organizations` - Company data
- `/entities/funding_rounds` - Funding details
- `/entities/people` - Founder info
- `/searches/organizations` - Advanced search

**Rate Limits:**
- Free tier: 200 requests/day
- Pro tier: 5,000 requests/day

**Sample Request:**
```python
headers = {
    'X-cb-user-key': api_key,
    'Content-Type': 'application/json'
}

response = requests.post(
    'https://api.crunchbase.com/api/v4/searches/organizations',
    headers=headers,
    json=query_payload
)
```

#### 2. **PitchBook API**

**Endpoints:**
- `/companies` - Company profiles
- `/valuations` - Valuation history
- `/deals` - M&A and funding deals

**Authentication:**
```python
headers = {
    'Authorization': f'Bearer {access_token}',
    'Accept': 'application/json'
}
```

#### 3. **Market Data API**

**Metrics Retrieved:**
- S&P 500 index
- NASDAQ composite
- VC funding index
- Interest rates
- IPO activity

**Update Frequency:** Daily

---

### Error Handling Strategy

**Retry Logic:**
```python
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds

for attempt in range(MAX_RETRIES):
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        if attempt < MAX_RETRIES - 1:
            time.sleep(RETRY_DELAY * (attempt + 1))
        else:
            logger.error(f"Failed after {MAX_RETRIES} attempts")
            return None
```

**Rate Limiting:**
```python
def _check_rate_limit(self):
    if self.request_count >= MAX_REQUESTS_PER_MINUTE:
        sleep_time = 60 - (time.time() - self.last_request_time)
        time.sleep(max(0, sleep_time))
        self.request_count = 0
```

---

## ğŸ—„ï¸ Database Schema

### Entity-Relationship Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STARTUPS   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)     â”‚â”€â”€â”
â”‚ external_id â”‚  â”‚
â”‚ name        â”‚  â”‚
â”‚ sector      â”‚  â”‚
â”‚ country     â”‚  â”‚
â”‚ stage       â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚             â”‚              â”‚
        â–¼                 â–¼             â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚FUNDING_ROUNDSâ”‚ â”‚ PREDICTIONS  â”‚ â”‚FINANCIALâ”‚ â”‚   CLUSTER    â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚ METRICS â”‚ â”‚ ASSIGNMENTS  â”‚
â”‚ id (PK)      â”‚ â”‚ id (PK)      â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ startup_id   â”‚ â”‚ startup_id   â”‚ â”‚ id (PK) â”‚ â”‚ id (PK)      â”‚
â”‚ amount_usd   â”‚ â”‚ predicted_   â”‚ â”‚startup  â”‚ â”‚ startup_id   â”‚
â”‚ valuation    â”‚ â”‚ valuation    â”‚ â”‚ _id     â”‚ â”‚ cluster_id   â”‚
â”‚ funding_date â”‚ â”‚ growth_rate  â”‚ â”‚ revenue â”‚ â”‚ cluster_name â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Table Specifications

**startups** (Main entity)
- Primary Key: `id` (auto-increment)
- Indexes: `external_id`, `name`, `sector`, `stage`
- Records: ~10,000

**funding_rounds**
- Foreign Key: `startup_id` â†’ startups(id)
- Indexes: `startup_id`, `funding_date`
- Records: ~25,000

**predictions**
- Foreign Key: `startup_id` â†’ startups(id)
- Contains ML model outputs
- Records: Updated daily

---

## ğŸ“ˆ Power BI Dashboards

### Dashboard Suite (4 Dashboards)

#### 1. **Executive Overview**
**Purpose:** High-level metrics for C-suite

**KPIs:**
- Total startups analyzed
- Total funding deployed ($466M)
- Average growth rate (94%)
- Active investors (245)

**Visualizations:**
- Funding trends (line chart)
- Top 10 startups (bar chart)
- Sector distribution (donut chart)
- Geographic map

#### 2. **Cluster Analysis**
**Purpose:** ML-driven segmentation insights

**Visualizations:**
- Scatter plot (growth vs funding)
- Cluster characteristics table
- Distribution by sector
- Cluster evolution over time

#### 3. **Investment Intelligence**
**Purpose:** Deal sourcing & due diligence

**Features:**
- High-potential targets (filtered list)
- Risk assessment dashboard
- Sector trends
- Competitive landscape

#### 4. **Predictive Analytics**
**Purpose:** Forward-looking insights

**Visualizations:**
- Actual vs predicted valuations
- Growth trajectory forecasts
- Success probability gauges
- Market sentiment indicators

---

## ğŸš€ Deployment

### Production Environment

**Server Requirements:**
- CPU: 8 cores minimum
- RAM: 16GB minimum
- Storage: 500GB SSD
- OS: Ubuntu 20.04 LTS

### Docker Deployment

**Dockerfile:**
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "main_pipeline.py"]
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: startup_analytics
      POSTGRES_USER: analytics_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  etl_pipeline:
    build: .
    depends_on:
      - postgres
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
    volumes:
      - ./data:/app/data

volumes:
  postgres_data:
```

**Deploy:**
```bash
docker-compose up -d
```

---

### Automated Scheduling

**Cron Job (Linux):**
```bash
# Run pipeline daily at 2 AM
0 2 * * * cd /path/to/project && /path/to/venv/bin/python main_pipeline.py >> logs/pipeline.log 2>&1
```

**Windows Task Scheduler:**
```powershell
# Create scheduled task
$action = New-ScheduledTaskAction -Execute "python" -Argument "main_pipeline.py"
$trigger = New-ScheduledTaskTrigger -Daily -At 2am
Register-ScheduledTask -Action $action -Trigger $trigger -TaskName "StartupETL"
```

---

## ğŸ”§ Maintenance

### Monitoring

**Key Metrics to Track:**
1. Pipeline execution time
2. API success rate
3. Database size
4. Model accuracy drift
5. Dashboard load time

**Logging Configuration:**
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler()
    ]
)
```

### Model Retraining

**Schedule:** Monthly or when accuracy drops below 0.80

**Process:**
1. Load latest data (last 6 months)
2. Retrain both models
3. Validate on holdout set
4. Compare metrics to previous version
5. Deploy if improved

### Database Maintenance

**Weekly Tasks:**
```sql
-- Vacuum to reclaim space
VACUUM ANALYZE startups;
VACUUM ANALYZE funding_rounds;

-- Reindex for performance
REINDEX TABLE startups;
REINDEX TABLE funding_rounds;

-- Update statistics
ANALYZE;
```

---

## ğŸ“š Additional Resources

### Documentation
- [API Integration Guide](api_integration.md)
- [Database Schema Reference](database_schema.md)
- [Power BI Setup](powerbi_integration.md)
- [ML Model Details](ml_models.md)

### Support
- **Email:** analytics@yourcompany.com
- **Slack:** #data-analytics
- **GitHub:** github.com/your-org/startup-analysis

---

**Version:** 1.0.0  
**Last Updated:** November 2024  
**License:** MIT
